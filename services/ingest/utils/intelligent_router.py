"""
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —ç–Ω–µ—Ä–≥–æ–∞—É–¥–∏—Ç–∞.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —è–≤–ª—è–µ—Ç—Å—è "–º–æ–∑–≥–æ–º" —Å–∏—Å—Ç–µ–º—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–º –õ–Æ–ë–´–ï –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
(Word, Excel, PDF, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è) –∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ (2-3 —Å–µ–∫) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (3-5 —Å–µ–∫) –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è routing map —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏ –∫–∞–∫ execution layer
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤ –∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤
"""

import logging
import json
import time
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç safe_json_dumps –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ datetime
try:
    from database import safe_json_dumps
except ImportError:
    # Fallback –µ—Å–ª–∏ database –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω
    def safe_json_dumps(obj, **kwargs):
        return json.dumps(obj, default=str, **kwargs)

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π
try:
    from utils.ai_content_classifier import AIContentClassifier
    HAS_AI_CLASSIFIER = True
except ImportError:
    HAS_AI_CLASSIFIER = False
    logger.warning("AIContentClassifier –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

try:
    from ai_parser import get_ai_parser
    HAS_AI_PARSER = True
except ImportError:
    HAS_AI_PARSER = False
    logger.warning("AI parser –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

try:
    from file_parser import parse_file
    HAS_FILE_PARSER = True
except ImportError:
    HAS_FILE_PARSER = False
    logger.warning("file_parser –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")


class IntelligentRouter:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª—é–±–æ–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    
    # –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    DOCUMENT_TYPES = [
        "energy_passport",      # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Å–ø–æ—Ä—Ç
        "balance_act",          # –ê–∫—Ç –±–∞–ª–∞–Ω—Å–∞
        "consumption_table",    # –¢–∞–±–ª–∏—Ü–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è
        "calculation",          # –†–∞—Å—á–µ—Ç
        "contract",             # –î–æ–≥–æ–≤–æ—Ä
        "protocol",             # –ü—Ä–æ—Ç–æ–∫–æ–ª
        "methodological",       # –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        "photo_thermogram",     # –§–æ—Ç–æ—Ç–µ—Ä–º–æ–≥—Ä–∞–º–º–∞
        "unknown"               # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø
    ]
    
    # –¢–∏–ø—ã —Ä–µ—Å—É—Ä—Å–æ–≤
    RESOURCE_TYPES = [
        "electricity",          # –≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏—è
        "gas",                  # –ì–∞–∑
        "water",                # –í–æ–¥–∞
        "heat",                 # –¢–µ–ø–ª–æ
        "fuel",                 # –¢–æ–ø–ª–∏–≤–æ
        "multiple",             # –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ—Å—É—Ä—Å–æ–≤
        "unknown"               # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ—Å—É—Ä—Å
    ]
    
    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    DATA_TYPES = [
        "meter_readings",       # –ü–æ–∫–∞–∑–∞–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤
        "energy_balance",       # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–∞–Ω—Å
        "savings_calculation",  # –†–∞—Å—á–µ—Ç —ç–∫–æ–Ω–æ–º–∏–∏
        "tariffs",              # –¢–∞—Ä–∏—Ñ—ã
        "norms",                # –ù–æ—Ä–º—ã
        "consumption",          # –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ
        "production",           # –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ
        "realization",          # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è
        "unknown"               # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
    ]
    
    # –ü–µ—Ä–∏–æ–¥—ã
    PERIOD_TYPES = [
        "2024_Q1", "2024_Q2", "2024_Q3", "2024_Q4",
        "2023_year", "2024_year",
        "multiyear",            # –ú–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
        "unknown"               # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥
    ]
    
    # –°—Ç–∞—Ç—É—Å—ã –¥–∞–Ω–Ω—ã—Ö
    STATUS_TYPES = [
        "source_data",          # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        "calculated",           # –†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        "reported",             # –û—Ç—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        "methodological"        # –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
    ]
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        self.ai_classifier = None
        self.ai_parser = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if HAS_AI_CLASSIFIER:
            try:
                self.ai_classifier = AIContentClassifier()
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {e}")
        
        if HAS_AI_PARSER:
            try:
                self.ai_parser = get_ai_parser()
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AI –ø–∞—Ä—Å–µ—Ä: {e}")
        
        logger.info("‚úÖ IntelligentRouter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze_file(
        self,
        file_path: str,
        filename: str,
        raw_json: Optional[Dict[str, Any]] = None,
        fast_mode: bool = True
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç routing map.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            raw_json: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            fast_mode: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ (True) –∏–ª–∏ –≥–ª—É–±–æ–∫–∏–π (False)
        
        Returns:
            Routing map —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
        """
        start_time = time.time()
        
        logger.info(f"üîç –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞: {filename}")
        
        # –≠—Ç–∞–ø 1: –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑
        fast_analysis = self._fast_analysis(file_path, filename, raw_json)
        confidence = fast_analysis.get("confidence", 0.0)
        
        # –†–µ—à–µ–Ω–∏–µ –æ –ø–µ—Ä–µ—Ö–æ–¥–µ –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É
        if not fast_mode and confidence < 0.7:
            logger.info(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ({confidence:.2f}), –ø–µ—Ä–µ—Ö–æ–¥ –∫ –≥–ª—É–±–æ–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É")
            deep_analysis = self._deep_analysis(file_path, filename, raw_json)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            analysis = {**fast_analysis, **deep_analysis}
            analysis["confidence"] = max(confidence, deep_analysis.get("confidence", 0.0))
        else:
            analysis = fast_analysis
        
        # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è routing map
        routing_map = self._generate_routing_map(analysis, file_path, filename)
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed_time:.2f} —Å–µ–∫. Confidence: {analysis.get('confidence', 0.0):.2f}")
        
        return routing_map
    
    def _fast_analysis(
        self,
        file_path: str,
        filename: str,
        raw_json: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ (2-3 —Å–µ–∫).
        
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–µ—Ä–≤—ã–µ –ª–∏—Å—Ç—ã/—Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
        """
        start_time = time.time()
        
        analysis = {
            "document_type": "unknown",
            "resource_type": "unknown",
            "data_type": "unknown",
            "period": "unknown",
            "status": "source_data",
            "confidence": 0.0,
            "metadata": {},
            "structure": {}
        }
        
        try:
            # –ï—Å–ª–∏ raw_json —É–∂–µ –µ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if raw_json is None:
                # –ü–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –ª–∏—Å—Ç—ã/—Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                raw_json = self._parse_file_preview(file_path)
            
            if not raw_json:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return analysis
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            structure = self._analyze_structure(raw_json, filename)
            analysis["structure"] = structure
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_type = self._detect_document_type(raw_json, filename)
            analysis["document_type"] = document_type
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä–µ—Å—É—Ä—Å–∞
            resource_type = self._detect_resource_type(raw_json, filename)
            analysis["resource_type"] = resource_type
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
            data_type = self._detect_data_type(raw_json, filename)
            analysis["data_type"] = data_type
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞
            period = self._detect_period(raw_json, filename)
            analysis["period"] = period
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
            status = self._detect_status(raw_json, filename)
            analysis["status"] = status
            
            # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = self._calculate_confidence(analysis)
            analysis["confidence"] = confidence
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            analysis["metadata"] = {
                "filename": filename,
                "file_size": Path(file_path).stat().st_size if Path(file_path).exists() else 0,
                "file_extension": Path(filename).suffix.lower(),
                "analysis_time": time.time() - start_time,
                "analysis_type": "fast"
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
            analysis["error"] = str(e)
        
        elapsed = time.time() - start_time
        logger.debug(f"–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫")
        
        return analysis
    
    def _deep_analysis(
        self,
        file_path: str,
        filename: str,
        raw_json: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ (3-5 —Å–µ–∫).
        
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Ç–æ—á–Ω–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏.
        """
        start_time = time.time()
        
        analysis = {
            "document_type": "unknown",
            "resource_type": "unknown",
            "data_type": "unknown",
            "period": "unknown",
            "status": "source_data",
            "confidence": 0.0,
            "anomalies": [],
            "errors": [],
            "recommendations": []
        }
        
        try:
            # –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞
            if raw_json is None:
                if HAS_FILE_PARSER:
                    raw_json = parse_file(file_path)
                else:
                    logger.warning("file_parser –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                    return analysis
            
            if not raw_json:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
                return analysis
            
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            analysis["document_type"] = self._detect_document_type(raw_json, filename)
            analysis["resource_type"] = self._detect_resource_type(raw_json, filename)
            analysis["data_type"] = self._detect_data_type(raw_json, filename)
            analysis["period"] = self._detect_period(raw_json, filename)
            analysis["status"] = self._detect_status(raw_json, filename)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
            analysis["anomalies"] = self._detect_anomalies(raw_json)
            analysis["errors"] = self._detect_errors(raw_json)
            analysis["recommendations"] = self._generate_recommendations(analysis, raw_json)
            
            # –ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            base_confidence = self._calculate_confidence(analysis)
            analysis["confidence"] = min(1.0, base_confidence * 1.2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–∞ 20%
            
            analysis["metadata"] = {
                "filename": filename,
                "analysis_time": time.time() - start_time,
                "analysis_type": "deep"
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
            analysis["error"] = str(e)
        
        elapsed = time.time() - start_time
        logger.debug(f"–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫")
        
        return analysis
    
    def _parse_file_preview(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–µ–≤—å—é —Ñ–∞–π–ª–∞ (–ø–µ—Ä–≤—ã–µ –ª–∏—Å—Ç—ã/—Å—Ç—Ä–∞–Ω–∏—Ü—ã) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
        """
        try:
            if HAS_FILE_PARSER:
                # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª (–º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–µ–≤—å—é)
                return parse_file(file_path)
            return None
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø—Ä–µ–≤—å—é: {e}")
            return None
    
    def _analyze_structure(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        structure = {
            "sheets_count": 0,
            "pages_count": 0,
            "tables_count": 0,
            "has_images": False,
            "sheet_names": [],
            "headers": []
        }
        
        try:
            # –î–ª—è Excel —Ñ–∞–π–ª–æ–≤
            if "sheets" in raw_json:
                sheets = raw_json["sheets"]
                structure["sheets_count"] = len(sheets)
                structure["sheet_names"] = [s.get("name", "") for s in sheets]
                
                # –ü–æ–¥—Å—á–µ—Ç —Ç–∞–±–ª–∏—Ü
                for sheet in sheets:
                    if "data" in sheet:
                        structure["tables_count"] += 1
                    if "headers" in sheet:
                        structure["headers"].extend(sheet["headers"])
            
            # –î–ª—è PDF —Ñ–∞–π–ª–æ–≤
            if "pages" in raw_json:
                structure["pages_count"] = len(raw_json["pages"])
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if "images" in raw_json or "has_images" in raw_json:
                structure["has_images"] = True
        
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
        
        return structure
    
    def _detect_document_type(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        filename_lower = filename.lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ raw_json (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑ OCR –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {"file_type": "...", "parsing": {"data": {"text": "..."}}}
        text_content = ""
        if isinstance(raw_json, dict):
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {"file_type": "...", "parsing": {"data": {...}}}
            if "parsing" in raw_json and isinstance(raw_json["parsing"], dict):
                parsing_data = raw_json["parsing"].get("data", {})
                if isinstance(parsing_data, dict):
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–ª–µ–π
                    if "text" in parsing_data and parsing_data["text"]:
                        text_content = str(parsing_data["text"]).lower()
                    elif "ocr_text" in parsing_data and parsing_data["ocr_text"]:
                        text_content = str(parsing_data["ocr_text"]).lower()
                    elif "ocr" in parsing_data and isinstance(parsing_data["ocr"], dict):
                        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                        ocr_result = parsing_data["ocr"]
                        if "text" in ocr_result and ocr_result["text"]:
                            text_content = str(ocr_result["text"]).lower()
            
            # –î–ª—è –ø—Ä—è–º–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã {"data": {...}}
            if not text_content and "data" in raw_json:
                data = raw_json["data"]
                if isinstance(data, dict):
                    if "text" in data and data["text"]:
                        text_content = str(data["text"]).lower()
                    elif "ocr_text" in data and data["ocr_text"]:
                        text_content = str(data["ocr_text"]).lower()
                    elif "ocr" in data and isinstance(data["ocr"], dict):
                        ocr_result = data["ocr"]
                        if "text" in ocr_result and ocr_result["text"]:
                            text_content = str(ocr_result["text"]).lower()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if not text_content:
                text_content = safe_json_dumps(raw_json).lower()
        else:
            text_content = safe_json_dumps(raw_json).lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        is_image = filename_lower.endswith(('.jpg', '.jpeg', '.png'))
        
        # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –ø—Ä–æ–≤–µ—Ä—è–µ–º OCR-—Ç–µ–∫—Å—Ç –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if is_image:
            # –¢–µ—Ä–º–æ–≥—Ä–∞–º–º—ã
            if any(keyword in filename_lower or keyword in text_content 
                   for keyword in ["—Ç–µ—Ä–º–æ–≥—Ä–∞–º–º", "thermogram", "—Ç–µ–ø–ª–æ–≤", "–∏–Ω—Ñ—Ä–∞–∫—Ä–∞—Å"]):
                return "photo_thermogram"
            
            # –ü–æ–∫–∞–∑–∞–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤ (—á–∞—Å—Ç–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä—É—é—Ç—Å—è)
            if any(keyword in filename_lower or keyword in text_content 
                   for keyword in ["—Å—á–µ—Ç—á–∏–∫", "meter", "–ø–æ–∫–∞–∑–∞–Ω–∏—è", "—Ç-3", "—Ç3", "—Ç-3–∞"]):
                return "meter_readings"
            
            # –ê–∫—Ç –±–∞–ª–∞–Ω—Å–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω)
            if any(keyword in filename_lower or keyword in text_content 
                   for keyword in ["–∞–∫—Ç –±–∞–ª–∞–Ω—Å–∞", "–±–∞–ª–∞–Ω—Å", "balance act", "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"]):
                return "balance_act"
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å OCR-—Ç–µ–∫—Å—Ç, –Ω–æ —Ç–∏–ø –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω - –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–∏—è
            if text_content and len(text_content) > 50:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —á–∏—Å–µ–ª –∏ –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è
                if any(keyword in text_content for keyword in ["–∫–≤—Ç—á", "–∫–≤—Ç", "–º¬≥", "–º3", "–≥–∫–∞–ª"]):
                    return "meter_readings"
        
        # –ü—Ä–∞–≤–∏–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤)
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Å–ø–æ—Ä—Ç", "—ç–Ω–µ—Ä–≥–æ–ø–∞—Å–ø–æ—Ä—Ç", "energy passport"]):
            return "energy_passport"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–∞–∫—Ç –±–∞–ª–∞–Ω—Å–∞", "–±–∞–ª–∞–Ω—Å", "balance act", "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"]):
            return "balance_act"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—Ç–∞–±–ª–∏—Ü–∞ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è", "consumption table", "–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ"]):
            return "consumption_table"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—Ä–∞—Å—á–µ—Ç", "calculation", "calc"]):
            return "calculation"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–¥–æ–≥–æ–≤–æ—Ä", "contract"]):
            return "contract"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–ø—Ä–æ—Ç–æ–∫–æ–ª", "protocol"]):
            return "protocol"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–º–µ—Ç–æ–¥–∏—á", "methodological"]):
            return "methodological"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—Ç–µ—Ä–º–æ–≥—Ä–∞–º–º", "thermogram"]):
            return "photo_thermogram"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.ai_classifier and self.ai_classifier.enabled:
            try:
                resource_type, confidence = self.ai_classifier.classify_with_ai(raw_json, filename)
                if confidence > 0.7:
                    # AI –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    pass
            except Exception as e:
                logger.debug(f"AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ —Å–º–æ–≥ –ø–æ–º–æ—á—å: {e}")
        
        return "unknown"
    
    def _detect_resource_type(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ä–µ—Å—É—Ä—Å–∞"""
        filename_lower = filename.lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ raw_json (–º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑ OCR –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        text_content = ""
        if isinstance(raw_json, dict):
            # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å OCR
            if "parsing" in raw_json and isinstance(raw_json["parsing"], dict):
                parsing_data = raw_json["parsing"].get("data", {})
                if "text" in parsing_data:
                    text_content = parsing_data["text"].lower()
                elif "ocr_text" in parsing_data:
                    text_content = parsing_data["ocr_text"].lower()
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            elif "data" in raw_json:
                data = raw_json["data"]
                if isinstance(data, dict):
                    if "text" in data:
                        text_content = data["text"].lower()
                    elif "ocr_text" in data:
                        text_content = data["ocr_text"].lower()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
            if not text_content:
                text_content = safe_json_dumps(raw_json).lower()
        else:
            text_content = safe_json_dumps(raw_json).lower()
        
        # –ü—Ä–∞–≤–∏–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ä–µ—Å—É—Ä—Å–∞
        resource_keywords = {
            "electricity": ["—ç–ª–µ–∫—Ç—Ä", "electricity", "–∫–≤—Ç—á", "–∫–≤—Ç", "kwh", "kw", "—ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏—è", "—Ç-3", "—Ç3"],
            "gas": ["–≥–∞–∑", "gas", "–º¬≥", "–º3", "–∫—É–±–æ–º–µ—Ç—Ä", "–∫—É–±"],
            "water": ["–≤–æ–¥–∞", "water", "–≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ"],
            "heat": ["—Ç–µ–ø–ª–æ", "heat", "–æ—Ç–æ–ø–ª–µ–Ω–∏–µ", "–≥–∫–∞–ª"],
            "fuel": ["—Ç–æ–ø–ª–∏–≤–æ", "fuel", "–Ω–µ—Ñ—Ç—å", "–±–µ–Ω–∑–∏–Ω"]
        }
        
        found_resources = []
        for resource_type, keywords in resource_keywords.items():
            if any(keyword in filename_lower or keyword in text_content for keyword in keywords):
                found_resources.append(resource_type)
        
        if len(found_resources) == 1:
            return found_resources[0]
        elif len(found_resources) > 1:
            return "multiple"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        if self.ai_classifier and self.ai_classifier.enabled:
            try:
                resource_type, confidence = self.ai_classifier.classify_with_ai(raw_json, filename)
                if confidence > 0.5 and resource_type:
                    return resource_type
            except Exception as e:
                logger.debug(f"AI –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ—Å—É—Ä—Å: {e}")
        
        return "unknown"
    
    def _detect_data_type(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö"""
        filename_lower = filename.lower()
        text_content = safe_json_dumps(raw_json).lower()
        
        # –ü—Ä–∞–≤–∏–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        if "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è" in filename_lower or "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è" in text_content:
            return "realization"
        
        if "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ" in filename_lower or "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ" in text_content:
            return "production"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–ø–æ–∫–∞–∑–∞–Ω–∏—è", "meter", "—Å—á–µ—Ç—á–∏–∫"]):
            return "meter_readings"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–±–∞–ª–∞–Ω—Å", "balance"]):
            return "energy_balance"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—ç–∫–æ–Ω–æ–º–∏—è", "savings", "—Ä–∞—Å—á–µ—Ç"]):
            return "savings_calculation"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—Ç–∞—Ä–∏—Ñ", "tariff"]):
            return "tariffs"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–Ω–æ—Ä–º–∞", "norm"]):
            return "norms"
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ
        if "–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ" in filename_lower or "–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ" in text_content:
            return "consumption"
        
        return "unknown"
    
    def _detect_period(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö"""
        import re
        
        filename_lower = filename.lower()
        text_content = safe_json_dumps(raw_json).lower()
        
        # –ü–æ–∏—Å–∫ –≥–æ–¥–∞
        year_match = re.search(r'20\d{2}', filename_lower + " " + text_content)
        year = year_match.group() if year_match else None
        
        # –ü–æ–∏—Å–∫ –∫–≤–∞—Ä—Ç–∞–ª–∞
        quarter_match = re.search(r'q[1-4]|–∫–≤[1-4]|–∫–≤–∞—Ä—Ç–∞–ª[_\s]*[1-4]', filename_lower + " " + text_content)
        quarter = quarter_match.group() if quarter_match else None
        
        if year and quarter:
            quarter_num = re.search(r'[1-4]', quarter).group() if quarter else None
            if quarter_num:
                return f"{year}_Q{quarter_num}"
        
        if year:
            return f"{year}_year"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–º–Ω–æ–≥–æ–ª–µ—Ç", "multiyear", "–Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç"]):
            return "multiyear"
        
        return "unknown"
    
    def _detect_status(
        self,
        raw_json: Dict[str, Any],
        filename: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –¥–∞–Ω–Ω—ã—Ö"""
        filename_lower = filename.lower()
        text_content = safe_json_dumps(raw_json).lower()
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–∏—Å—Ö–æ–¥–Ω", "source", "–ø–µ—Ä–≤–∏—á–Ω"]):
            return "source_data"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["—Ä–∞—Å—Å—á–∏—Ç–∞–Ω", "calculated", "calc"]):
            return "calculated"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–æ—Ç—á–µ—Ç", "reported", "report"]):
            return "reported"
        
        if any(keyword in filename_lower or keyword in text_content 
               for keyword in ["–º–µ—Ç–æ–¥–∏—á", "methodological"]):
            return "methodological"
        
        return "source_data"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –∞–Ω–∞–ª–∏–∑–∞"""
        confidence = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if analysis.get("document_type") != "unknown":
            confidence += 0.3
        
        if analysis.get("resource_type") != "unknown":
            confidence += 0.3
        
        if analysis.get("data_type") != "unknown":
            confidence += 0.2
        
        if analysis.get("period") != "unknown":
            confidence += 0.2
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤
        document_type = analysis.get("document_type")
        if document_type in ["meter_readings", "photo_thermogram"]:
            # –î–ª—è –ø–æ–∫–∞–∑–∞–Ω–∏–π —Å—á–µ—Ç—á–∏–∫–æ–≤ –∏ —Ç–µ—Ä–º–æ–≥—Ä–∞–º–º –ø–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence += 0.1
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        structure = analysis.get("structure", {})
        if structure.get("tables_count", 0) > 0 or structure.get("pages_count", 0) > 0:
            confidence += 0.1
        
        return min(1.0, confidence)
    
    def _detect_anomalies(self, raw_json: Dict[str, Any]) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        anomalies = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –ª–∏—Å—Ç—ã
            if "sheets" in raw_json:
                empty_sheets = [s.get("name", "") for s in raw_json["sheets"] 
                              if not s.get("data") or len(s.get("data", [])) == 0]
                if empty_sheets:
                    anomalies.append(f"–ü—É—Å—Ç—ã–µ –ª–∏—Å—Ç—ã: {', '.join(empty_sheets)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            if "sheets" in raw_json:
                sheets_without_headers = [s.get("name", "") for s in raw_json["sheets"]
                                         if "headers" not in s or not s["headers"]]
                if sheets_without_headers:
                    anomalies.append(f"–õ–∏—Å—Ç—ã –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {', '.join(sheets_without_headers)}")
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
        
        return anomalies
    
    def _detect_errors(self, raw_json: Dict[str, Any]) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        errors = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if "sheets" in raw_json:
                for sheet in raw_json["sheets"]:
                    if "data" in sheet:
                        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        pass
        
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫: {e}")
        
        return errors
    
    def _generate_recommendations(
        self,
        analysis: Dict[str, Any],
        raw_json: Dict[str, Any]
    ) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
        recommendations = []
        
        if analysis.get("confidence", 0.0) < 0.7:
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑-–∑–∞ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
        
        if analysis.get("document_type") == "unknown":
            recommendations.append("–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        if analysis.get("resource_type") == "unknown":
            recommendations.append("–¢–∏–ø —Ä–µ—Å—É—Ä—Å–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        return recommendations
    
    def _generate_routing_map(
        self,
        analysis: Dict[str, Any],
        file_path: str,
        filename: str
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç routing map —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ.
        
        –§–æ—Ä–º–∞—Ç routing map:
        {
            "file_info": {...},
            "analysis": {...},
            "routing": {
                "primary_module": "...",
                "secondary_modules": [...],
                "target_tables": [...],
                "processing_priority": "...",
                "validation_required": bool
            },
            "metadata": {...}
        }
        """
        routing_map = {
            "file_info": {
                "filename": filename,
                "file_path": file_path,
                "uploaded_at": datetime.now().isoformat()
            },
            "analysis": analysis,
            "routing": self._determine_routing(analysis),
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "router_version": "1.0.0"
            }
        }
        
        return routing_map
    
    def _determine_routing(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞.
        
        –í—ã–±–∏—Ä–∞–µ—Ç –º–æ–¥—É–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã –ë–î.
        """
        document_type = analysis.get("document_type", "unknown")
        resource_type = analysis.get("resource_type", "unknown")
        data_type = analysis.get("data_type", "unknown")
        confidence = analysis.get("confidence", 0.0)
        
        routing = {
            "primary_module": "manual_review",
            "secondary_modules": [],
            "target_tables": [],
            "processing_priority": "normal",
            "validation_required": True
        }
        
        # –ï—Å–ª–∏ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if confidence < 0.7:
            routing["primary_module"] = "manual_review"
            routing["processing_priority"] = "low"
            return routing
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ primary_module –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if document_type == "balance_act":
            routing["primary_module"] = "balance_sheet_node_extractor"
            routing["target_tables"] = ["node_consumption"]
            if data_type in ["realization", "production"]:
                routing["secondary_modules"] = ["energy_aggregator"]
        
        elif document_type == "energy_passport":
            routing["primary_module"] = "canonical_to_passport"
            routing["target_tables"] = ["parsed_data"]
            routing["secondary_modules"] = ["readiness_validator"]
        
        elif document_type == "consumption_table":
            routing["primary_module"] = "nodes_parser"
            routing["target_tables"] = ["node_consumption"]
            routing["secondary_modules"] = ["energy_aggregator"]
        
        elif document_type == "meter_readings":
            # –ü–æ–∫–∞–∑–∞–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤ (–≤–∫–ª—é—á–∞—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            routing["primary_module"] = "file_parser"  # OCR —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω
            routing["target_tables"] = ["parsed_data"]
            routing["secondary_modules"] = ["ocr_data_adapter"]  # –ê–¥–∞–ø—Ç–∞—Ü–∏—è OCR –¥–∞–Ω–Ω—ã—Ö
            routing["processing_priority"] = "normal"
        
        elif document_type == "photo_thermogram":
            # –§–æ—Ç–æ—Ç–µ—Ä–º–æ–≥—Ä–∞–º–º—ã
            routing["primary_module"] = "file_parser"
            routing["target_tables"] = ["parsed_data"]
            routing["processing_priority"] = "low"
        
        elif document_type in ["calculation", "savings_calculation"]:
            routing["primary_module"] = "energy_aggregator"
            routing["target_tables"] = ["parsed_data"]
        
        elif document_type == "methodological":
            routing["primary_module"] = "file_parser"
            routing["target_tables"] = ["parsed_data"]
            routing["processing_priority"] = "low"
        
        else:
            # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ - –∏—Å–ø–æ–ª—å–∑—É–µ–º file_parser
            routing["primary_module"] = "file_parser"
            routing["target_tables"] = ["parsed_data", "uploads"]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        if document_type in ["balance_act", "consumption_table"]:
            routing["processing_priority"] = "high"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if document_type in ["energy_passport", "balance_act"]:
            routing["validation_required"] = True
        else:
            routing["validation_required"] = False
        
        return routing
    
    def route_file(
        self,
        file_path: str,
        filename: str,
        enterprise_id: int,
        batch_id: str,
        raw_json: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: –∞–Ω–∞–ª–∏–∑ + –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è + –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            enterprise_id: ID –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è
            batch_id: ID –±–∞—Ç—á–∞
            raw_json: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å routing map –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        """
        # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
        routing_map = self.analyze_file(file_path, filename, raw_json, fast_mode=True)
        
        # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
        if routing_map["analysis"].get("confidence", 0.0) < 0.7:
            routing_map = self.analyze_file(file_path, filename, raw_json, fast_mode=False)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
        routing_map["execution"] = {
            "enterprise_id": enterprise_id,
            "batch_id": batch_id,
            "status": "pending",
            "executed_modules": []
        }
        
        return routing_map

