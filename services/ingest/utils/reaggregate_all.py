"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑĞµÑ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¾Ğº.
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğº ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼.
"""

import logging
from pathlib import Path
from typing import Dict, Any

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹
import sys

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from utils.energy_aggregator import (
    aggregate_from_db_json,
    write_aggregation_json,
)
import database
import os

# ĞŸÑƒÑ‚ÑŒ Ğº Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ingest
ingest_path = Path(__file__).resolve().parent.parent
INBOX_DIR = Path(os.getenv("INBOX_DIR", str(ingest_path / "data" / "inbox")))
AGGREGATED_DIR = Path(os.getenv("AGGREGATED_DIR", str(INBOX_DIR / "aggregated")))
AGGREGATED_DIR.mkdir(parents=True, exist_ok=True)


def reaggregate_enterprise(enterprise_id: int) -> Dict[str, Any]:
    """
    ĞŸĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ.

    Args:
        enterprise_id: ID Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ

    Returns:
        Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿ĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸
    """
    logger.info(f"ğŸ”„ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ID: {enterprise_id}")

    uploads = database.list_uploads_for_enterprise(enterprise_id)
    stats = {"processed": 0, "success": 0, "failed": 0, "aggregated_files": []}

    for upload in uploads:
        batch_id = upload.get("batch_id")
        filename = upload.get("filename", "unknown")
        status = upload.get("status")

        if status != "success" or not batch_id:
            continue

        logger.info(f"ğŸ“„ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ°: {filename} (batch_id: {batch_id[:8]}...)")
        stats["processed"] += 1

        try:
            upload_record = database.get_upload_by_batch(batch_id)
            if not upload_record or not upload_record.get("raw_json"):
                logger.warning(f"âš ï¸ ĞĞµÑ‚ raw_json Ğ´Ğ»Ñ {filename}")
                stats["failed"] += 1
                continue

            raw_json = upload_record["raw_json"]

            # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            aggregation_data = aggregate_from_db_json(raw_json)

            if not aggregation_data:
                logger.warning(f"âš ï¸ ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ Ğ´Ğ»Ñ {filename}")
                stats["failed"] += 1
                continue

            # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ñ„Ğ°Ğ¹Ğ» pererashod)
            # Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ² aggregate_from_db_json, ĞµÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞµÑÑ‚ÑŒ

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            aggregated_file = write_aggregation_json(
                batch_id, aggregation_data, AGGREGATED_DIR
            )
            stats["aggregated_files"].append(str(aggregated_file))
            stats["success"] += 1

            logger.info(f"âœ… ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {aggregated_file.name}")

            # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ
            if "resources" in aggregation_data:
                resources = aggregation_data["resources"]
                for resource_name, resource_data in resources.items():
                    if resource_data:
                        quarters = list(resource_data.keys())
                        logger.info(
                            f"  ğŸ“Š {resource_name}: {len(quarters)} ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»Ğ¾Ğ² - {quarters[:3]}..."
                        )

        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ {filename}: {e}", exc_info=True)
            stats["failed"] += 1

    logger.info(
        f"âœ… ĞŸĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°: Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾={stats['processed']}, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾={stats['success']}, Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº={stats['failed']}"
    )
    return stats


def reaggregate_all() -> None:
    """ĞŸĞµÑ€ĞµĞ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¹"""
    enterprises = database.list_enterprises()

    logger.info(f"ğŸ”„ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¹: {len(enterprises)}")

    for enterprise in enterprises:
        enterprise_id = enterprise["id"]
        enterprise_name = enterprise["name"]
        logger.info(f"\n{'=' * 60}")
        logger.info(f"ğŸ¢ ĞŸÑ€ĞµĞ´Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ: {enterprise_name} (ID: {enterprise_id})")
        logger.info(f"{'=' * 60}")

        stats = reaggregate_enterprise(enterprise_id)

        logger.info(f"ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ {enterprise_name}:")
        logger.info(f"   ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {stats['processed']}")
        logger.info(f"   Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾: {stats['success']}")
        logger.info(f"   ĞÑˆĞ¸Ğ±Ğ¾Ğº: {stats['failed']}")


if __name__ == "__main__":
    import os
    import sys

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼
    ingest_path = Path(__file__).resolve().parent.parent
    if str(ingest_path) not in sys.path:
        sys.path.insert(0, str(ingest_path))

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    if len(sys.argv) > 1:
        enterprise_id = int(sys.argv[1])
        reaggregate_enterprise(enterprise_id)
    else:
        reaggregate_all()
