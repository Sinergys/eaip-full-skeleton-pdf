"""–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∏–º–ø–æ—Ä—Ç–∞ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
import json
import sqlite3
from pathlib import Path
from datetime import datetime

DB_PATH = Path(__file__).resolve().parent.parent / "ingest_data.db"
AGGREGATED_DIR = Path(__file__).resolve().parent.parent / "data" / "inbox" / "aggregated"

def check_database():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ë–î"""
    print("=" * 70)
    print("–ü–†–û–í–ï–†–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•")
    print("=" * 70)
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü
    cursor.execute("SELECT COUNT(*) FROM enterprises")
    enterprises = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM uploads")
    uploads = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM parsed_data")
    parsed = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM aggregated_data")
    aggregated = cursor.fetchone()[0]
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î:")
    print(f"   –ü—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π: {enterprises}")
    print(f"   –ó–∞–≥—Ä—É–∑–æ–∫: {uploads}")
    print(f"   –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {parsed}")
    print(f"   –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {aggregated}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã aggregated_data
    cursor.execute("PRAGMA table_info(aggregated_data)")
    columns = cursor.fetchall()
    print(f"\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã aggregated_data:")
    for col in columns:
        print(f"   {col[1]:20} | {col[2]:15} | NOT NULL: {bool(col[3])}")
    
    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
    cursor.execute("""
        SELECT batch_id, filename, status, created_at 
        FROM uploads 
        ORDER BY created_at DESC 
        LIMIT 5
    """)
    print(f"\nüìÅ –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∑–∞–≥—Ä—É–∑–æ–∫:")
    for row in cursor.fetchall():
        print(f"   {row[0][:8]}... | {row[1][:40]:40} | {row[2]:10}")
    
    conn.close()
    return {
        "enterprises": enterprises,
        "uploads": uploads,
        "parsed": parsed,
        "aggregated": aggregated
    }

def check_aggregated_files():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ aggregated/"""
    print("\n" + "=" * 70)
    print("–ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í –í aggregated/")
    print("=" * 70)
    
    if not AGGREGATED_DIR.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {AGGREGATED_DIR}")
        return {}
    
    files = list(AGGREGATED_DIR.glob("*_aggregated.json"))
    print(f"\nüìÇ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    
    files_with_data = []
    files_empty = []
    files_with_errors = []
    
    for f in files:
        try:
            with open(f, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            resources = data.get("resources", {})
            has_data = False
            
            for resource_type, resource_data in resources.items():
                if isinstance(resource_data, dict) and resource_data:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–µ—Ä–∏–æ–¥—ã —Å –¥–∞–Ω–Ω—ã–º–∏
                    for period, period_data in resource_data.items():
                        if isinstance(period_data, dict):
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                            for key, value in period_data.items():
                                if isinstance(value, (int, float)) and value != 0:
                                    has_data = True
                                    break
                            if has_data:
                                break
                    if has_data:
                        break
            
            if has_data:
                files_with_data.append(f.name)
            else:
                files_empty.append(f.name)
        except Exception as e:
            files_with_errors.append((f.name, str(e)))
    
    print(f"\n‚úÖ –§–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏: {len(files_with_data)}")
    if files_with_data:
        print("   –ü—Ä–∏–º–µ—Ä—ã:")
        for fname in files_with_data[:3]:
            print(f"     - {fname}")
    
    print(f"\n‚ö†Ô∏è –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã: {len(files_empty)}")
    if files_empty:
        print("   –ü—Ä–∏–º–µ—Ä—ã:")
        for fname in files_empty[:3]:
            print(f"     - {fname}")
    
    if files_with_errors:
        print(f"\n‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏: {len(files_with_errors)}")
        for fname, error in files_with_errors[:3]:
            print(f"     - {fname}: {error}")
    
    return {
        "total": len(files),
        "with_data": len(files_with_data),
        "empty": len(files_empty),
        "errors": len(files_with_errors)
    }

def check_import_code():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –∏–º–ø–æ—Ä—Ç–∞"""
    print("\n" + "=" * 70)
    print("–ü–†–û–í–ï–†–ö–ê –ö–û–î–ê –ò–ú–ü–û–†–¢–ê")
    print("=" * 70)
    
    main_py = Path(__file__).resolve().parent.parent / "main.py"
    
    if not main_py.exists():
        print("‚ùå –§–∞–π–ª main.py –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    with open(main_py, 'r', encoding='utf-8') as f:
        content = f.read()
    
    has_import = "import_resource_to_db" in content
    has_try_except = "try:" in content and "import_exc" in content
    
    print(f"\n‚úÖ –ò–º–ø–æ—Ä—Ç –≤ –∫–æ–¥–µ:")
    print(f"   –í—ã–∑–æ–≤ import_resource_to_db: {'‚úÖ –î–∞' if has_import else '‚ùå –ù–µ—Ç'}")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫: {'‚úÖ –î–∞' if has_try_except else '‚ùå –ù–µ—Ç'}")
    
    # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∏–º–ø–æ—Ä—Ç–æ–º
    lines = content.split('\n')
    import_lines = []
    for i, line in enumerate(lines, 1):
        if "import_resource_to_db" in line:
            import_lines.append((i, line.strip()[:80]))
    
    if import_lines:
        print(f"\n   –ù–∞–π–¥–µ–Ω–æ –≤—ã–∑–æ–≤–æ–≤: {len(import_lines)}")
        for line_num, line_content in import_lines[:3]:
            print(f"     –°—Ç—Ä–æ–∫–∞ {line_num}: {line_content}...")

def check_logs():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤"""
    print("\n" + "=" * 70)
    print("–ü–†–û–í–ï–†–ö–ê –õ–û–ì–û–í")
    print("=" * 70)
    
    log_file = Path(__file__).resolve().parent.parent / "logs" / "aggregation_events.jsonl"
    
    if not log_file.exists():
        print("‚ö†Ô∏è –§–∞–π–ª –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    events = []
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    events.append(json.loads(line))
                except:
                    pass
    
    print(f"\nüìù –°–æ–±—ã—Ç–∏–π –≤ –ª–æ–≥–µ: {len(events)}")
    
    if events:
        success = sum(1 for e in events if e.get("status") == "success")
        failed = sum(1 for e in events if e.get("status") == "error")
        
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö: {success}")
        print(f"   ‚ùå –û—à–∏–±–æ–∫: {failed}")
        
        print(f"\n   –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–æ–±—ã—Ç–∏—è:")
        for event in events[-3:]:
            print(f"     {event.get('timestamp', '')[:19]} | {event.get('status', '')} | {event.get('batch_id', '')[:8]}...")

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("–ü–û–õ–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–¢–ê–¢–£–°–ê –ò–ú–ü–û–†–¢–ê –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–• –î–ê–ù–ù–´–•")
    print("=" * 70)
    
    db_stats = check_database()
    file_stats = check_aggregated_files()
    check_import_code()
    check_logs()
    
    print("\n" + "=" * 70)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
    print("=" * 70)
    print(f"\nüìä –ë–î:")
    print(f"   –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {db_stats['aggregated']}")
    print(f"   –ó–∞–≥—Ä—É–∑–æ–∫: {db_stats['uploads']}")
    
    print(f"\nüìÇ –§–∞–π–ª—ã:")
    print(f"   –í—Å–µ–≥–æ: {file_stats.get('total', 0)}")
    print(f"   –° –¥–∞–Ω–Ω—ã–º–∏: {file_stats.get('with_data', 0)}")
    print(f"   –ü—É—Å—Ç—ã–µ: {file_stats.get('empty', 0)}")
    
    print(f"\nüîç –í–´–í–û–î:")
    if db_stats['aggregated'] == 0:
        print("   ‚ö†Ô∏è –í –ë–î –Ω–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        if file_stats.get('with_data', 0) > 0:
            print("   ‚úÖ –ù–æ –µ—Å—Ç—å —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ - –Ω—É–∂–µ–Ω batch-–∏–º–ø–æ—Ä—Ç")
        else:
            print("   ‚ö†Ô∏è –ò –≤—Å–µ —Ñ–∞–π–ª—ã –ø—É—Å—Ç—ã–µ - –≤–æ–∑–º–æ–∂–Ω–æ, –∏–º–ø–æ—Ä—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    else:
        print("   ‚úÖ –ò–º–ø–æ—Ä—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç, –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å –≤ –ë–î")
    
    print("=" * 70)

